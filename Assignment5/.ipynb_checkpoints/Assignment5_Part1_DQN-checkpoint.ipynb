{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br>Assignment #5 Part 1: Implementing and Training a Deep Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Hyungyu Lee, November 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement one of famous reinforcement learning algorithm, Deep Q-Network (DQN) of DeepMind. <br>\n",
    "The goal here is to understand a basic form of DQN [1, 2] and learn how to use OpenAI Gym toolkit [3].<br>\n",
    "You need to follow the instructions to implement the given classes.\n",
    "\n",
    "1. [Play](#play) ( 50 points )\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **two parts of the assignment**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].tar.gz*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* &nbsp; Team_#)\n",
    "\n",
    "### Some helpful references for assignment #4 :\n",
    "- [1] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013). [[pdf]](https://www.google.co.kr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwiI3aqPjavVAhXBkJQKHZsIDpgQFgg7MAI&url=https%3A%2F%2Fwww.cs.toronto.edu%2F~vmnih%2Fdocs%2Fdqn.pdf&usg=AFQjCNEd1AJoM72DeDpI_GBoPuv7NnVoFA)\n",
    "- [2] Mnih, Volodymyr, et al. \"Human-level control through deep reinforcement learning.\" Nature 518.7540 (2015): 529-533. [[pdf]](https://www.nature.com/nature/journal/v518/n7540/pdf/nature14236.pdf)\n",
    "- [3] OpenAI GYM website [[link]](https://gym.openai.com/envs) and [[git]](https://github.com/openai/gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. OpenAI Gym\n",
    "\n",
    "OpenAI Gym is a toolkit to support diverse environments for developing reinforcement learning algorithms. You can use the toolkit with Python as well as TensorFlow. Installation guide of OpenAI Gym is offered by [this link](https://github.com/openai/gym#installation) or just type the command \"pip install gym\" (as well as \"pip install gym[atari]\" for Part2). \n",
    "\n",
    "After you set up OpenAI Gym, you can use APIs of the toolkit by inserting <font color=red>import gym</font> into your code. In this assignment, you must build one of famous reinforcement learning algorithms whose agent can run on OpenAI Gym environments. Please check how to use APIs such as funcions interacting with environments in the followings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2 \n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-03 14:50:26,694] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "# Make an environment instance of CartPole-v0.\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Before interacting with the environment and starting a new episode, you must reset the environment's state.\n",
    "state = env.reset()\n",
    "\n",
    "#rendering game screens, do not need for assignment evaluation\n",
    "# env.render() \n",
    "\n",
    "# You can check action space and state (observation) space.\n",
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "print(num_actions)\n",
    "print(state_shape)\n",
    "\n",
    "# \"step\" function performs agent's actions given current state of the environment and returns several values.\n",
    "# Input: action (numerical data)\n",
    "#        - env.action_space.sample(): select a random action among possible actions.\n",
    "# Output: next_state (numerical data, next state of the environment after performing given action)\n",
    "#         reward (numerical data, reward of given action given current state)\n",
    "#         terminal (boolean data, True means the agent is done in the environment)\n",
    "next_state, reward, terminal, info = env.step(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement a DQN agent\n",
    "## 1) Overview of implementation in the notebook\n",
    "\n",
    "The assignment is based on a method named by Deep Q-Network (DQN) [1,2]. You could find the details of DQN in the papers. The followings show briefly architecture of DQN and its training computation flow.\n",
    "\n",
    "- (Pink flow) Play an episode and save transition records of the episode into a replay memory.\n",
    "- (Green flow) Train DQN so that a loss function in the figure is minimized. The loss function is computed using main Q-network and Target Q-network. Target Q-network needs to be periodically updated by copying the main Q-network.\n",
    "- (Purple flow) Gradient can be autonomously computed by tensorflow engine, if you build a proper optimizer.\n",
    "\n",
    "![](image/architecture.png)\n",
    "\n",
    "There are major 4 components, each of which needs to be implemented in this notebook. The Agent class must have an instance(s) of each class (Environment, DQN, ReplayMemory).\n",
    "- Environment\n",
    "- DQN \n",
    "- ReplayMemory\n",
    "- Agent\n",
    "\n",
    "![](image/components.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Design classes\n",
    "\n",
    "In the code cells, there are only names of functions which are used in TA's implementation and their brief explanations. <font color='green'>...</font> means that the functions need more arguments and <font color='green'>pass</font> means that you need to write more codes. The functions may be helpful when you do not know how to start the assignment. Of course, you could change the functions such as deleting/adding functions or extending/reducing roles of the classes, <font color='red'> just keeping the existence of the classes</font>.\n",
    "\n",
    "### Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pikinder/DQN\n",
    "#https://github.com/TangLaoDA/DQN_FOR_CartPole-v0/blob/master/game_CartPole_train.py\n",
    "\n",
    "tf.reset_default_graph()\n",
    "EXPERIENCE_REPLAY_BATCH = 64\n",
    "EXPERIENCE_BUFFER_SIZE = 100000\n",
    "START_EPSILON = 0.99\n",
    "END_EPSILON = 0.1\n",
    "EPSILON_STEP_LIMIT = 100\n",
    "LEARNING_RATE = 0.0025\n",
    "DISCOUNT_FACTORE = 0.99\n",
    "ALPHA = 1\n",
    "class Environment(object):\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "    def step(self,action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReplayMemory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self):\n",
    "        self.experience = []\n",
    "        self.visited = {}\n",
    "\n",
    "    def remember(self, state, next_state, action, reward, is_done):\n",
    "        state = np.array(state, dtype=np.float64)\n",
    "        next_state = np.array(next_state, dtype=np.float64)\n",
    "        experience = (state, next_state, action, reward, is_done)\n",
    "        if len(self.experience) > EXPERIENCE_BUFFER_SIZE:\n",
    "            self.experience = self.experience[1:]\n",
    "\n",
    "        self.experience.append(experience)\n",
    "\n",
    "    def recall(self):\n",
    "        experience_size = len(self.experience)\n",
    "        _EXPERIENCE_REPLAY_BATCH = EXPERIENCE_REPLAY_BATCH\n",
    "        if experience_size < EXPERIENCE_REPLAY_BATCH:\n",
    "            _EXPERIENCE_REPLAY_BATCH = experience_size\n",
    "\n",
    "        indexes = np.random.randint(\n",
    "            experience_size, size=_EXPERIENCE_REPLAY_BATCH)\n",
    "        experiences = []\n",
    "        for index in indexes:\n",
    "            experiences.append(self.experience[index])\n",
    "\n",
    "        return experiences\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.replay_buffer = deque()\n",
    "    \n",
    "    def add(self, state, action, reward, next_action, done):            \n",
    "        one_hot_action = np.zeros(self.action_dim)\n",
    "        one_hot_action[action] = 1\n",
    "        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:\n",
    "            self.replay_buffer.popleft()\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:\n",
    "            self.train_Q_network()\n",
    "        # Add current_state, action, reward, terminal to replay_memory (next_state which can be added by your choice). \n",
    "        pass\n",
    "    \n",
    "    def mini_batch(self, batch_size):\n",
    "        return random.sample(self.replay_buffer,BATCH_SIZE)\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "    def count(self):\n",
    "    # if buffer is full, return buffer size\n",
    "    # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0\n",
    "        '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.count = 0\n",
    "\n",
    "        x = tf.placeholder(tf.float64, [None, input_size], name='x')\n",
    "        y = tf.placeholder(tf.float64, [None, output_size], name='y')\n",
    "\n",
    "        w1 = tf.Variable(tf.random_normal(\n",
    "            [input_size, 64], dtype=tf.float64), name='w1')\n",
    "        b1 = tf.Variable(tf.random_normal([64], dtype=tf.float64), name='b1')\n",
    "        tf.summary.histogram('w1', w1)\n",
    "\n",
    "        w2 = tf.Variable(tf.random_normal(\n",
    "            [64, output_size], dtype=tf.float64), name='w2')\n",
    "        b2 = tf.Variable(tf.random_normal(\n",
    "            [output_size], dtype=tf.float64), name='b2')\n",
    "        tf.summary.histogram('w2', w2)\n",
    "\n",
    "        h1 = tf.add(tf.matmul(x, w1), b1, name='h1')\n",
    "        relu_h1 = tf.nn.tanh(h1, name='relu_h1')\n",
    "        tf.summary.histogram('relu_h1', relu_h1)\n",
    "\n",
    "        self.model = tf.add(tf.matmul(relu_h1, w2), b2, name='model')\n",
    "        tf.summary.histogram('model', self.model)\n",
    "\n",
    "        self.error = tf.reduce_mean(tf.square(self.model - y), name='error')\n",
    "        tf.summary.scalar('error', self.error)\n",
    "\n",
    "        self.optimzer = tf.train.RMSPropOptimizer(\n",
    "            LEARNING_RATE, name='Optimizer').minimize(self.error)\n",
    "        self.step = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.summary_writter = tf.summary.FileWriter(\n",
    "            \"/tmp/cart_pole\", self.sess.graph)\n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess.run(self.init)\n",
    "    def get_action(self, state):\n",
    "        state = np.array(state, dtype=np.float64)\n",
    "        output = self.sess.run([self.model], feed_dict={\n",
    "            'x:0': state\n",
    "        })\n",
    "        return output[0][0]\n",
    "\n",
    "    def train(self, states, actions):\n",
    "        states = np.array(states, dtype=np.float64)\n",
    "        actions = np.array(actions, dtype=np.float64)\n",
    "        summary, _, error = self.sess.run([self.merged, self.optimzer, self.error], feed_dict={\n",
    "            'x:0': states,\n",
    "            'y:0': actions\n",
    "        })\n",
    "        self.summary_writter.add_summary(summary, self.count)\n",
    "        self.count += 1\n",
    "        # print error\n",
    "        # sess.close()\n",
    "        return error\n",
    "\n",
    "    def get_action_multiple(self, states):\n",
    "        states = np.array(states, dtype=np.float64)\n",
    "        output = self.sess.run([self.model], feed_dict={\n",
    "            'x:0': states\n",
    "        })\n",
    "        # sess.close()\n",
    "        return output[0]\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        \n",
    "        '''\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON\n",
    "        self.state_shape = env.observation_space.shape[0]\n",
    "        self.num_actions = env.action_space.n \n",
    "        self.prediction_Q = self.build_network('pred')\n",
    "        self.target_Q = self.build_network('target')\n",
    "        '''\n",
    "        '''\n",
    "    def build_network(self,name):\n",
    "        with tf.name_scope(name):\n",
    "        # Make your a deep neural network\n",
    "            W1 = self.weight_variable([self.state_dim,20])\n",
    "            b1 = self.bias_variable([20])\n",
    "            W2 = self.weight_variable([20,self.action_dim])\n",
    "            b2 = self.bias_variable([self.action_dim])\n",
    "            # input layer\n",
    "            self.state_input = tf.placeholder(\"float\",[None,self.state_dim])\n",
    "            # hidden layers\n",
    "            h_layer = tf.nn.relu(tf.matmul(self.state_input,W1) + b1)\n",
    "            # Q Value layer\n",
    "            self.Q_value = tf.matmul(h_layer,W2) + b2\n",
    "            \n",
    "            # update target network with Q network\n",
    "            copy_op = []\n",
    "            pred_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='pred')\n",
    "            target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='target')\n",
    "            for pred_var, target_var in zip(pred_vars, target_vars):\n",
    "                copy_op.append(target_var.assign(pred_var.value()))\n",
    "                \n",
    "    def build_optimizer(self):\n",
    "        # Make your optimizer \n",
    "        self.action_input = tf.placeholder(\"float\",[None,self.action_dim]) # one hot presentation\n",
    "        self.y_input = tf.placeholder(\"float\",[None])\n",
    "        Q_action = tf.reduce_sum(tf.multiply(self.Q_value,self.action_input),reduction_indices = 1)\n",
    "        self.cost = tf.reduce_mean(tf.square(self.y_input - Q_action))\n",
    "        tf.summary.scalar(\"loss\",self.cost)\n",
    "        global merged_summary_op\n",
    "        merged_summary_op = tf.summary.merge_all()\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.0001).minimize(self.cost)\n",
    "        \n",
    "    def train_network(self):\n",
    "        # Train the prediction_Q network using a mini-batch sampled from the replay memory\n",
    "        pass\n",
    "    \n",
    "    def update_target_network(self, ...):\n",
    "        self.sess.run(copy_op)\n",
    "    \n",
    "    def predict_Q(self, ...):\n",
    "        pass\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-03 14:51:22,182] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running episode: 1\n",
      "rewards: 14.0, step: 14\n",
      "running episode: 2\n",
      "rewards: 19.0, step: 33\n",
      "running episode: 3\n",
      "rewards: 23.0, step: 56\n",
      "running episode: 4\n",
      "rewards: 17.0, step: 73\n",
      "running episode: 5\n",
      "rewards: 12.0, step: 85\n",
      "running episode: 6\n",
      "rewards: 11.0, step: 96\n",
      "running episode: 7\n",
      "rewards: 17.0, step: 113\n",
      "running episode: 8\n",
      "rewards: 54.0, step: 167\n",
      "running episode: 9\n",
      "rewards: 18.0, step: 185\n",
      "running episode: 10\n",
      "rewards: 12.0, step: 197\n",
      "running episode: 11\n",
      "rewards: 36.0, step: 233\n",
      "running episode: 12\n",
      "rewards: 31.0, step: 264\n",
      "running episode: 13\n",
      "rewards: 13.0, step: 277\n",
      "running episode: 14\n",
      "rewards: 21.0, step: 298\n",
      "running episode: 15\n",
      "rewards: 16.0, step: 314\n",
      "running episode: 16\n",
      "rewards: 19.0, step: 333\n",
      "running episode: 17\n",
      "rewards: 17.0, step: 350\n",
      "running episode: 18\n",
      "rewards: 35.0, step: 385\n",
      "running episode: 19\n",
      "rewards: 32.0, step: 417\n",
      "running episode: 20\n",
      "rewards: 33.0, step: 450\n",
      "running episode: 21\n",
      "rewards: 10.0, step: 460\n",
      "running episode: 22\n",
      "rewards: 43.0, step: 503\n",
      "running episode: 23\n",
      "rewards: 16.0, step: 519\n",
      "running episode: 24\n",
      "rewards: 33.0, step: 552\n",
      "running episode: 25\n"
     ]
    },
    {
     "ename": "ArgumentError",
     "evalue": "argument 2: <class 'TypeError'>: wrong type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArgumentError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-30ddb48c2422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilons\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m '''\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-30ddb48c2422>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-d8987be704d9>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgeom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-19/lib/python3.6/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mdispatch_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;31m# Check for the events specific to this window\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXCheckWindowEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0x1ffffff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             \u001b[0;31m# Key events are filtered by the xlib window event\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;31m# handler so they get a shot at the prefiltered event.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mArgumentError\u001b[0m: argument 2: <class 'TypeError'>: wrong type"
     ]
    }
   ],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, epsilons,total_episodes):\n",
    "        #self.saver = tf.train.Saver()\n",
    "        env = gym.make(\"CartPole-v0\")\n",
    "        self.env = Environment(env)\n",
    "        self.total_episodes = total_episodes\n",
    "        self.epsilons = epsilons\n",
    "        self.epsilons_index = 0\n",
    "        self.epsilon = START_EPSILON\n",
    "        self.total_actions = 0\n",
    "        self.total_greedy_actions = 0\n",
    "        self.model = DQN(4, 2)\n",
    "        self.memory = ReplayMemory()\n",
    "        self.step = 0\n",
    "        self.avg = []\n",
    "        \n",
    "       \n",
    "    @staticmethod\n",
    "    def is_greddy(epsilon):\n",
    "        return np.random.choice([0, 1], 1, p=[epsilon, 1 - epsilon])[0]\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        index = int(self.total_actions / EPSILON_STEP_LIMIT)\n",
    "        if index > len(self.epsilons - 1):\n",
    "            index = len(self.epsilons) - 1\n",
    "\n",
    "        self.epsilons_index = index\n",
    "\n",
    "    def take_action(self, state):\n",
    "        \"\"\"\n",
    "        actions are whether you want to go right or left\n",
    "        \"\"\"\n",
    "        self.total_actions += 1\n",
    "        q_values = self.model.get_action(state.reshape(1, 4))\n",
    "        is_greedy = Agent.is_greddy(self.epsilon)\n",
    "        msg = ''\n",
    "        if is_greedy:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = np.random.choice([0, 1], 1)[0]\n",
    "            msg = 'explorer'\n",
    "\n",
    "        self.epsilon = END_EPSILON + \\\n",
    "            (START_EPSILON - END_EPSILON) * \\\n",
    "            math.exp(-0.001 * self.total_actions)\n",
    "        return action\n",
    "\n",
    "    def observe_results(self, state, next_state, action, reward, is_done):\n",
    "        \"\"\"\n",
    "        after taking action environment return result of it, store (state, action, reward, is_done) in memory \n",
    "        for experience replay\n",
    "        \"\"\"\n",
    "        self.memory.remember(state, next_state, action, reward, is_done)\n",
    "        self.update()\n",
    "\n",
    "    def close(self):\n",
    "        return self.model.close()\n",
    "\n",
    "    def update(self):\n",
    "        experiences = self.memory.recall()\n",
    "        current_states = None\n",
    "        next_states = None\n",
    "        for experience in experiences:\n",
    "            current_state, next_state, action, reward, is_done = experience\n",
    "            current_state = np.array(current_state).reshape(1, 4)\n",
    "            next_state = np.array(next_state).reshape(1, 4)\n",
    "            if current_states is None:\n",
    "                current_states = current_state\n",
    "                next_states = next_state\n",
    "            else:\n",
    "                current_states = np.vstack((current_states, current_state))\n",
    "                next_states = np.vstack((next_states, next_state))\n",
    "\n",
    "        current_state_q_values = self.model.get_action_multiple(current_states)\n",
    "        next_state_q_values = self.model.get_action_multiple(next_states)\n",
    "\n",
    "        x = None\n",
    "        y = None\n",
    "        for i in range(len(experiences)):\n",
    "            current_state, next_state, action, reward, is_done = experiences[i]\n",
    "            current_state_q_value = np.array(\n",
    "                current_state_q_values[i], dtype=np.float64)\n",
    "            next_state_q_value = np.array(\n",
    "                next_state_q_values[i], dtype=np.float64)\n",
    "            if is_done:\n",
    "                reward = -10\n",
    "                next_state_q_value = [0.0, 0.0]\n",
    "\n",
    "            current_state_q_value[action] = ALPHA * \\\n",
    "                (reward + DISCOUNT_FACTORE * np.amax(next_state_q_value))\n",
    "\n",
    "            current_state = np.array(current_state).reshape(1, 4)\n",
    "            current_state_q_value = np.array(\n",
    "                current_state_q_value).reshape(1, 2)\n",
    "\n",
    "            if x is None:\n",
    "                x = current_state\n",
    "                y = current_state_q_value\n",
    "            else:\n",
    "                x = np.vstack((x, current_state))\n",
    "                y = np.vstack((y, current_state_q_value))\n",
    "\n",
    "        self.model.train(x, y)\n",
    " #============================================\n",
    "    def add_rewards(self, total_rewards):\n",
    "        self.avg.append(total_rewards)\n",
    "        l = len(self.avg)\n",
    "        if l < 100:\n",
    "            return False\n",
    "\n",
    "        _avg = float(sum(self.avg[l - 100: l])) / max(len(self.avg[l - 100: l]), 1)\n",
    "        print ('avg rewards: %s' % str(_avg))\n",
    "        if _avg > 195:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def run(self):\n",
    "        episodes = 0\n",
    "        #self.gym.wrappers.Monitor('results/cartpole', force=True)\n",
    "        while episodes < self.total_episodes:\n",
    "            print ('running episode: %s' % str(episodes + 1))\n",
    "            state = self.env.reset()\n",
    "            is_done = False\n",
    "            total_reward = 0\n",
    "            while not is_done:\n",
    "                self.env.render()\n",
    "                action = self.take_action(state)\n",
    "                next_state, reward, is_done, info = self.env.step(action)\n",
    "                self.step += 1\n",
    "                total_reward += reward\n",
    "                self.observe_results(\n",
    "                    state, next_state, action, reward, is_done)\n",
    "                state = next_state\n",
    "\n",
    "            print ('rewards: %s, step: %s' % (str(total_reward), str(self.step)))\n",
    "            if self.add_rewards(total_reward):\n",
    "                print ('done with episods %s and steps: %s' % (str(episodes), str(self.step)))\n",
    "                #self.env.monitor.close()\n",
    "                self.agent.close()\n",
    "                # self._plot()\n",
    "                return\n",
    "\n",
    "            episodes += 1\n",
    "\n",
    "    def _plot(self):\n",
    "        plt.plot(self.avg)\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.savefig('rewards.png')\n",
    "        plt.show()\n",
    "        \n",
    "epsilons = np.linspace(START_EPSILON, END_EPSILON)\n",
    "\n",
    "env = Agent(epsilons,50)\n",
    "env.run()\n",
    "'''\n",
    "    def select_action(self, ...):\n",
    "        # Select an action according ε-greedy. You need to use a random-number generating function and add a library if necessary.\n",
    "        pass\n",
    "    \n",
    "    def train(self, ...):\n",
    "        # Train your agent \n",
    "        # Several hyper-parameters are determined by your choice\n",
    "        # Keep epsilon-greedy action selection in your mind \n",
    "        pass\n",
    "    \n",
    "    def play(self, ...):\n",
    "        # Test your agent \n",
    "        # When performing test, you can show the environment's screen by rendering if you want\n",
    "        pass\n",
    "    \n",
    "    def save(self):\n",
    "        checkpoint_dir = 'cartpole'\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.mkdir(checkpoint_dir)\n",
    "        self.saver.save(self.sess, os.path.join(checkpoint_dir, 'trained_agent'))\n",
    "        \n",
    "    def load(self):\n",
    "        checkpoint_dir = 'cartpole'\n",
    "        self.saver.restore(self.sess, os.path.join(checkpoint_dir, 'trained_agent'))\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train your agent \n",
    "\n",
    "Now, you train an agent to play CartPole-v0. Options class is the collection of hyper-parameters that you can choice. Usage of Options class is not mandatory.<br>\n",
    "The maximum value of total reward which can be aquired from one episode is 200. \n",
    "<font color='red'>**You should show learning status such as the number of observed states and mean/max/min of rewards frequently (for instance, every 100 states).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8e46822690c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \"epsilons\": 0.99})\n\u001b[1;32m     23\u001b[0m     \u001b[0mmyAgent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# It depends on your class implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmyAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmyAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import easydict\n",
    "parser = argparse.ArgumentParser(description=\"CartPole\")\n",
    "parser.add_argument('--env-name', default='CartPole-v0', type=str,\n",
    "                    help=\"Environment\")\n",
    "parser.add_argument('--epsilons', default=0.99, type=float,\n",
    "                    help='1epsilons')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "\"\"\"\n",
    "You can add more arguments.\n",
    "for example, visualize, memory_size, batch_size, discount_factor, eps_max, eps_min, learning_rate, train_interval, copy_interval and so on\n",
    "\"\"\"\n",
    "with tf.Session(config=config) as sess:\n",
    "    #args = parser.parse_args()\n",
    "    args = easydict.EasyDict({\n",
    "        \"epsilons\": 0.99})\n",
    "    myAgent = Agent(args) # It depends on your class implementation\n",
    "    myAgent.train()\n",
    "    myAgent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"play\"></a> 3. Test the trained agent ( 50 points )\n",
    "\n",
    "Now, we test your agent and calculate an average reward of 20 episodes.\n",
    "- 0 <= average reward < 50 : you can get 0 points\n",
    "- 50 <= average reward < 100 : you can get 10 points\n",
    "- 100 <= average reward < 190 : you can get 35 points\n",
    "- 190 <= average reward <= 200 : you can get 50 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "# If you use a GPU, uncomment\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "# config.log_device_placement = False\n",
    "# config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=config) as sess:\n",
    "    args = parser.parse_args() # You set the option of test phase\n",
    "    myAgent = Agent(args, test) # It depends on your class implementation\n",
    "    myAgent.load()\n",
    "    rewards = []\n",
    "    for i in range(20):\n",
    "        r = myAgent.play() # play() returns the reward cumulated in one episode\n",
    "        rewards.append(r)\n",
    "    mean = np.mean(rewards)\n",
    "    print(rewards)\n",
    "    print(mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
