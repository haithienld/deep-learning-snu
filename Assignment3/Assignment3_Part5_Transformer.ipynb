{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Assignment #3 Part 5: Transformer\n",
    "\n",
    "Copyright (C) Data Science Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. Written by Jeonghee Jo, October 2019\n",
    "\n",
    "This is about Transformer (Vaswani et al., 2017).\n",
    "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
    "\n",
    "Original blog post & code:\n",
    "https://github.com/Kyubyong/transformer\n",
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "That said, you are allowed to copy paste the codes from the original repo.\n",
    "HOWEVER, <font color=red> try to implement the model yourself first </font>, and consider the original source code as a last resort.\n",
    "You will learn a lot while wrapping around your head during the implementation. And you will understand nuts and bolts of RNNs more clearly in a code level.\n",
    "\n",
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.  \n",
    "Once you have done **all Assignment Part 1-5**, run the *CollectSubmission.sh* script with your **Team number** as input argument. <br>\n",
    "This will produce a zipped file called *[Your team number].zip*. Please submit this file on ETL. &nbsp;&nbsp; (Usage: ./*CollectSubmission.sh* team_#)\n",
    "\n",
    "### Transformer (20 points)\n",
    "\n",
    "This assignment is an on/off one: just make this notebook **\"work\"** without problem by: \n",
    "\n",
    "1. **Explore various hyperparameters and pick the best set (in class Hparams, transformer_modules.py)** \n",
    "\n",
    "### The Grading is as follows:\n",
    "\n",
    "1. Train your model using at least <font color=red> 12 different hyperparameter set </font>. Report performance results (BLEU score) on given test set <font color=red> for corresponding each hyperparameter set </font>. \n",
    "\n",
    "2. Plus, <font color=red> submit the one checkpoint file </font> of your best model. \n",
    "\n",
    "The details are described in <font color=red>**transformer_modules.py**</font>. (There is nothing to implement in this notebook.)\n",
    "\n",
    "\n",
    "Now proceed to the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_utils import *\n",
    "from transformer_modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "import errno\n",
    "import sentencepiece as spm\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(hp):\n",
    "    \"\"\"Load raw data -> Preprocessing -> Segmenting with sentencepice\n",
    "    hp: hyperparams. argparse.\n",
    "    \"\"\"\n",
    "    \n",
    "    train1 = \"./iwslt2016/de-en/train.tags.de-en.de\"\n",
    "    train2 = \"./iwslt2016/de-en/train.tags.de-en.en\"\n",
    "    eval1 = \"./iwslt2016/de-en/IWSLT16.TED.tst2013.de-en.de.xml\"\n",
    "    eval2 = \"./iwslt2016/de-en/IWSLT16.TED.tst2013.de-en.en.xml\"\n",
    "    test1 = \"./iwslt2016/de-en/IWSLT16.TED.tst2014.de-en.de.xml\"\n",
    "    test2 = \"./iwslt2016/de-en/IWSLT16.TED.tst2014.de-en.en.xml\"\n",
    "    for f in (train1, train2, eval1, eval2, test1, test2):\n",
    "        if not os.path.isfile(f):\n",
    "            raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), f)\n",
    "\n",
    "    # train\n",
    "    _prepro = lambda x:  [line.strip() for line in open(x, 'r').read().split(\"\\n\") \\\n",
    "                      if not line.startswith(\"<\")]\n",
    "    prepro_train1, prepro_train2 = _prepro(train1), _prepro(train2)\n",
    "    assert len(prepro_train1)==len(prepro_train2), \"Check if train source and target files match.\"\n",
    "\n",
    "    # eval\n",
    "    _prepro = lambda x: [re.sub(\"<[^>]+>\", \"\", line).strip() \\\n",
    "                     for line in open(x, 'r').read().split(\"\\n\") \\\n",
    "                     if line.startswith(\"<seg id\")]\n",
    "    prepro_eval1, prepro_eval2 = _prepro(eval1), _prepro(eval2)\n",
    "    assert len(prepro_eval1) == len(prepro_eval2), \"Check if eval source and target files match.\"\n",
    "\n",
    "    # test\n",
    "    prepro_test1, prepro_test2 = _prepro(test1), _prepro(test2)\n",
    "    assert len(prepro_test1) == len(prepro_test2), \"Check if test source and target files match.\"\n",
    "\n",
    "    os.makedirs(\"./iwslt2016/prepro\", exist_ok=True)\n",
    "    def _write(sents, fname):\n",
    "        with open(fname, 'w') as fout:\n",
    "            fout.write(\"\\n\".join(sents))\n",
    "\n",
    "    _write(prepro_train1, \"./iwslt2016/prepro/train.de\")\n",
    "    _write(prepro_train2, \"./iwslt2016/prepro/train.en\")\n",
    "    _write(prepro_train1+prepro_train2, \"./iwslt2016/prepro/train\")\n",
    "    _write(prepro_eval1, \"./iwslt2016/prepro/eval.de\")\n",
    "    _write(prepro_eval2, \"./iwslt2016/prepro/eval.en\")\n",
    "    _write(prepro_test1, \"./iwslt2016/prepro/test.de\")\n",
    "    _write(prepro_test2, \"./iwslt2016/prepro/test.en\")\n",
    "\n",
    "    os.makedirs(\"./iwslt2016/segmented\", exist_ok=True)\n",
    "    train = '--input=./iwslt2016/prepro/train --pad_id=0 --unk_id=1 \\\n",
    "             --bos_id=2 --eos_id=3\\\n",
    "             --model_prefix=./iwslt2016/segmented/bpe --vocab_size={} \\\n",
    "             --model_type=bpe'.format(hp.vocab_size)\n",
    "    spm.SentencePieceTrainer.Train(train)\n",
    "\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(\"./iwslt2016/segmented/bpe.model\")\n",
    "\n",
    "    def _segment_and_write(sents, fname):\n",
    "        with open(fname, \"w\") as fout:\n",
    "            for sent in sents:\n",
    "                pieces = sp.EncodeAsPieces(sent)\n",
    "                fout.write(\" \".join(pieces) + \"\\n\")\n",
    "\n",
    "    _segment_and_write(prepro_train1, \"./iwslt2016/segmented/train.de.bpe\")\n",
    "    _segment_and_write(prepro_train2, \"./iwslt2016/segmented/train.en.bpe\")\n",
    "    _segment_and_write(prepro_eval1, \"./iwslt2016/segmented/eval.de.bpe\")\n",
    "    _segment_and_write(prepro_eval2, \"./iwslt2016/segmented/eval.en.bpe\")\n",
    "    _segment_and_write(prepro_test1, \"./iwslt2016/segmented/test.de.bpe\")\n",
    "\n",
    "    print(\"train1:\", open(\"./iwslt2016/segmented/train.de.bpe\",'r').readline())\n",
    "    print(\"train2:\", open(\"./iwslt2016/segmented/train.en.bpe\", 'r').readline())\n",
    "    print(\"eval1:\", open(\"./iwslt2016/segmented/eval.de.bpe\", 'r').readline())\n",
    "    print(\"eval2:\", open(\"./iwslt2016/segmented/eval.en.bpe\", 'r').readline())\n",
    "    print(\"test1:\", open(\"./iwslt2016/segmented/test.de.bpe\", 'r').readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Hparams()\n",
    "parser = hparams.parser\n",
    "hp = parser.parse_args()\n",
    "prepro(hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    '''\n",
    "    xs: tuple of\n",
    "        x: int32 tensor. (N, T1)\n",
    "        x_seqlens: int32 tensor. (N,)\n",
    "        sents1: str tensor. (N,)\n",
    "    ys: tuple of\n",
    "        decoder_input: int32 tensor. (N, T2)\n",
    "        y: int32 tensor. (N, T2)\n",
    "        y_seqlen: int32 tensor. (N, )\n",
    "        sents2: str tensor. (N,)\n",
    "    training: boolean.\n",
    "    '''\n",
    "    def __init__(self, hp):\n",
    "        self.hp = hp\n",
    "        self.token2idx, self.idx2token = load_vocab(hp.vocab)\n",
    "        self.embeddings = get_token_embeddings(self.hp.vocab_size, self.hp.d_model, zero_pad=True)\n",
    "\n",
    "    def encode(self, xs, training=True):\n",
    "        '''\n",
    "        Returns\n",
    "        memory: encoder outputs. (N, T1, d_model)\n",
    "        '''\n",
    "        with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "            x, seqlens, sents1 = xs\n",
    "\n",
    "            # src_masks\n",
    "            src_masks = tf.math.equal(x, 0) # (N, T1)\n",
    "\n",
    "            # embedding\n",
    "            enc = tf.nn.embedding_lookup(self.embeddings, x) # (N, T1, d_model)\n",
    "            enc *= self.hp.d_model**0.5 # scale\n",
    "\n",
    "            enc += positional_encoding(enc, self.hp.maxlen1)\n",
    "            enc = tf.layers.dropout(enc, self.hp.dropout_rate, training=training)\n",
    "\n",
    "            ## Blocks\n",
    "            for i in range(self.hp.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                    # self-attention\n",
    "                    enc = multihead_attention(queries=enc,\n",
    "                                              keys=enc,\n",
    "                                              values=enc,\n",
    "                                              key_masks=src_masks,\n",
    "                                              num_heads=self.hp.num_heads,\n",
    "                                              dropout_rate=self.hp.dropout_rate,\n",
    "                                              training=training,\n",
    "                                              causality=False)\n",
    "                    # feed forward\n",
    "                    enc = ff(enc, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "        memory = enc\n",
    "        return memory, sents1, src_masks\n",
    "\n",
    "    def decode(self, ys, memory, src_masks, training=True):\n",
    "        '''\n",
    "        memory: encoder outputs. (N, T1, d_model)\n",
    "        src_masks: (N, T1)\n",
    "        Returns\n",
    "        logits: (N, T2, V). float32.\n",
    "        y_hat: (N, T2). int32\n",
    "        y: (N, T2). int32\n",
    "        sents2: (N,). string.\n",
    "        '''\n",
    "        with tf.variable_scope(\"decoder\", reuse=tf.AUTO_REUSE):\n",
    "            decoder_inputs, y, seqlens, sents2 = ys\n",
    "\n",
    "            # tgt_masks\n",
    "            tgt_masks = tf.math.equal(decoder_inputs, 0)  # (N, T2)\n",
    "\n",
    "            # embedding\n",
    "            dec = tf.nn.embedding_lookup(self.embeddings, decoder_inputs)  # (N, T2, d_model)\n",
    "            dec *= self.hp.d_model ** 0.5  # scale\n",
    "\n",
    "            dec += positional_encoding(dec, self.hp.maxlen2)\n",
    "            dec = tf.layers.dropout(dec, self.hp.dropout_rate, training=training)\n",
    "\n",
    "            # Blocks\n",
    "            for i in range(self.hp.num_blocks):\n",
    "                with tf.variable_scope(\"num_blocks_{}\".format(i), reuse=tf.AUTO_REUSE):\n",
    "                    # Masked self-attention (Note that causality is True at this time)\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                                              keys=dec,\n",
    "                                              values=dec,\n",
    "                                              key_masks=tgt_masks,\n",
    "                                              num_heads=self.hp.num_heads,\n",
    "                                              dropout_rate=self.hp.dropout_rate,\n",
    "                                              training=training,\n",
    "                                              causality=True,\n",
    "                                              scope=\"self_attention\")\n",
    "\n",
    "                    # Vanilla attention\n",
    "                    dec = multihead_attention(queries=dec,\n",
    "                                              keys=memory,\n",
    "                                              values=memory,\n",
    "                                              key_masks=src_masks,\n",
    "                                              num_heads=self.hp.num_heads,\n",
    "                                              dropout_rate=self.hp.dropout_rate,\n",
    "                                              training=training,\n",
    "                                              causality=False,\n",
    "                                              scope=\"vanilla_attention\")\n",
    "                    ### Feed Forward\n",
    "                    dec = ff(dec, num_units=[self.hp.d_ff, self.hp.d_model])\n",
    "\n",
    "        # Final linear projection (embedding weights are shared)\n",
    "        weights = tf.transpose(self.embeddings) # (d_model, vocab_size)\n",
    "        logits = tf.einsum('ntd,dk->ntk', dec, weights) # (N, T2, vocab_size)\n",
    "        y_hat = tf.to_int32(tf.argmax(logits, axis=-1))\n",
    "\n",
    "        return logits, y_hat, y, sents2\n",
    "\n",
    "    def train(self, xs, ys):\n",
    "        '''\n",
    "        Returns\n",
    "        loss: scalar.\n",
    "        train_op: training operation\n",
    "        global_step: scalar.\n",
    "        summaries: training summary node\n",
    "        '''\n",
    "        # forward\n",
    "        memory, sents1, src_masks = self.encode(xs)\n",
    "        logits, preds, y, sents2 = self.decode(ys, memory, src_masks)\n",
    "\n",
    "        # train scheme\n",
    "        y_ = label_smoothing(tf.one_hot(y, depth=self.hp.vocab_size))\n",
    "        ce = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y_)\n",
    "        nonpadding = tf.to_float(tf.not_equal(y, self.token2idx[\"<pad>\"]))  # 0: <pad>\n",
    "        loss = tf.reduce_sum(ce * nonpadding) / (tf.reduce_sum(nonpadding) + 1e-7)\n",
    "\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        lr = noam_scheme(self.hp.lr, global_step, self.hp.warmup_steps)\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "        tf.summary.scalar('lr', lr)\n",
    "        tf.summary.scalar(\"loss\", loss)\n",
    "        tf.summary.scalar(\"global_step\", global_step)\n",
    "\n",
    "        summaries = tf.summary.merge_all()\n",
    "\n",
    "        return loss, train_op, global_step, summaries\n",
    "\n",
    "    def eval(self, xs, ys):\n",
    "        '''Predicts autoregressively\n",
    "        At inference, input ys is ignored.\n",
    "        Returns\n",
    "        y_hat: (N, T2)\n",
    "        '''\n",
    "        decoder_inputs, y, y_seqlen, sents2 = ys\n",
    "\n",
    "        decoder_inputs = tf.ones((tf.shape(xs[0])[0], 1), tf.int32) * self.token2idx[\"<s>\"]\n",
    "        ys = (decoder_inputs, y, y_seqlen, sents2)\n",
    "\n",
    "        memory, sents1, src_masks = self.encode(xs, False)\n",
    "\n",
    "        for _ in tqdm(range(self.hp.maxlen2)):\n",
    "            logits, y_hat, y, sents2 = self.decode(ys, memory, src_masks, False)\n",
    "            if tf.reduce_sum(y_hat, 1) == self.token2idx[\"<pad>\"]: break\n",
    "\n",
    "            _decoder_inputs = tf.concat((decoder_inputs, y_hat), 1)\n",
    "            ys = (_decoder_inputs, y, y_seqlen, sents2)\n",
    "\n",
    "        # monitor a random sample\n",
    "        n = tf.random_uniform((), 0, tf.shape(y_hat)[0]-1, tf.int32)\n",
    "        sent1 = sents1[n]\n",
    "        pred = convert_idx_to_token_tensor(y_hat[n], self.idx2token)\n",
    "        sent2 = sents2[n]\n",
    "\n",
    "        tf.summary.text(\"sent1\", sent1)\n",
    "        tf.summary.text(\"pred\", pred)\n",
    "        tf.summary.text(\"sent2\", sent2)\n",
    "        summaries = tf.summary.merge_all()\n",
    "\n",
    "        return y_hat, summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Hparams()\n",
    "parser = hparams.parser\n",
    "hp = parser.parse_args()\n",
    "save_hparams(hp, hp.logdir)\n",
    "\n",
    "train_batches, num_train_batches, num_train_samples = get_batch(hp.train1, hp.train2,\n",
    "                                             hp.maxlen1, hp.maxlen2,\n",
    "                                             hp.vocab, hp.batch_size,\n",
    "                                             shuffle=True)\n",
    "eval_batches, num_eval_batches, num_eval_samples = get_batch(hp.eval1, hp.eval2,\n",
    "                                             100000, 100000,\n",
    "                                             hp.vocab, hp.batch_size,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = tf.data.Iterator.from_structure(train_batches.output_types, train_batches.output_shapes)\n",
    "xs, ys = iter.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_init_op = iter.make_initializer(train_batches)\n",
    "eval_init_op = iter.make_initializer(eval_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Transformer(hp)\n",
    "loss, train_op, global_step, train_summaries = m.train(xs, ys)\n",
    "y_hat, eval_summaries = m.eval(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=hp.num_epochs)\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.latest_checkpoint(hp.logdir)\n",
    "    if ckpt is None:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        save_variable_specs(os.path.join(hp.logdir, \"specs\"))\n",
    "    else:\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "    summary_writer = tf.summary.FileWriter(hp.logdir, sess.graph)\n",
    "\n",
    "    sess.run(train_init_op)\n",
    "    total_steps = hp.num_epochs * num_train_batches\n",
    "    _gs = sess.run(global_step)\n",
    "    for i in tqdm(range(_gs, total_steps+1)):\n",
    "        _, _gs, _summary = sess.run([train_op, global_step, train_summaries])\n",
    "        epoch = math.ceil(_gs / num_train_batches)\n",
    "        summary_writer.add_summary(_summary, _gs)\n",
    "\n",
    "        if _gs and _gs % num_train_batches == 0:\n",
    "            _loss = sess.run(loss) # train loss\n",
    "\n",
    "            _, _eval_summaries = sess.run([eval_init_op, eval_summaries])\n",
    "            summary_writer.add_summary(_eval_summaries, _gs)\n",
    "\n",
    "            hypotheses = get_hypotheses(num_eval_batches, num_eval_samples, sess, y_hat, m.idx2token)\n",
    "\n",
    "            model_output = \"iwslt2016_E%02dL%.2f\" % (epoch, _loss)\n",
    "            if not os.path.exists(hp.evaldir): os.makedirs(hp.evaldir)\n",
    "            translation = os.path.join(hp.evaldir, model_output)\n",
    "            with open(translation, 'w') as fout:\n",
    "                fout.write(\"\\n\".join(hypotheses))\n",
    "\n",
    "            calc_bleu(hp.eval3, translation)\n",
    "\n",
    "            ckpt_name = os.path.join(hp.logdir, model_output)\n",
    "            saver.save(sess, ckpt_name, global_step=_gs)\n",
    "\n",
    "            sess.run(train_init_op)\n",
    "    summary_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_hparams(hp, hp.ckpt)\n",
    "\n",
    "test_batches, num_test_batches, num_test_samples  = get_batch(hp.test1, hp.test1,\n",
    "                                              100000, 100000,\n",
    "                                              hp.vocab, hp.test_batch_size,\n",
    "                                              shuffle=False)\n",
    "iter = tf.data.Iterator.from_structure(test_batches.output_types, test_batches.output_shapes)\n",
    "xs, ys = iter.get_next()\n",
    "\n",
    "test_init_op = iter.make_initializer(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat, _ = m.eval(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    ckpt_ = tf.train.latest_checkpoint(hp.ckpt)\n",
    "    ckpt = hp.ckpt if ckpt_ is None else ckpt_ # None: ckpt is a file. otherwise dir.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    saver.restore(sess, ckpt)\n",
    "\n",
    "    sess.run(test_init_op)\n",
    "\n",
    "    hypotheses = get_hypotheses(num_test_batches, num_test_samples, sess, y_hat, m.idx2token)\n",
    "\n",
    "    model_output = ckpt.split(\"/\")[-1]\n",
    "    if not os.path.exists(hp.testdir): os.makedirs(hp.testdir)\n",
    "    translation = os.path.join(hp.testdir, model_output)\n",
    "    with open(translation, 'w') as fout:\n",
    "        fout.write(\"\\n\".join(hypotheses))\n",
    "\n",
    "    calc_bleu(hp.test2, translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-19",
   "language": "python",
   "name": "deep-learning-19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
